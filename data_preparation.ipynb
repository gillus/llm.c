{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd204fca-c237-488a-ab4f-8602e033d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461cc28a-42af-46f8-979e-1fb8cbb864d3",
   "metadata": {},
   "source": [
    "# Download and extract data\n",
    "\n",
    "Download and extract a dump from Wikipedia\n",
    "```\n",
    "wget -O simplewiki-20250301-pages-articles-multistream.xml.bz2 https://dumps.wikimedia.org/simplewiki/20250301/simplewiki-20250301-pages-articles-multistream.xml.bz2\n",
    "```\n",
    "\n",
    "```\n",
    "bzip2 -d simplewiki-20250301-pages-articles-multistream.xml.bz2 \n",
    "```\n",
    "\n",
    "Use WikiExtractor to convert XML into a number of JSON files\n",
    "```\n",
    "python -m wikiextractor.WikiExtractor simplewiki-20250301-pages-articles-multistream.xml --json --output extracted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65bdc343-34ac-407f-9ab2-ea8324404de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../extracted/AA/wiki_28',\n",
       " '../extracted/AA/wiki_70',\n",
       " '../extracted/AA/wiki_72',\n",
       " '../extracted/AA/wiki_95',\n",
       " '../extracted/AA/wiki_09',\n",
       " '../extracted/AA/wiki_81',\n",
       " '../extracted/AA/wiki_99',\n",
       " '../extracted/AA/wiki_17',\n",
       " '../extracted/AA/wiki_03',\n",
       " '../extracted/AA/wiki_20',\n",
       " '../extracted/AA/wiki_00',\n",
       " '../extracted/AA/wiki_05',\n",
       " '../extracted/AA/wiki_47',\n",
       " '../extracted/AA/wiki_21',\n",
       " '../extracted/AA/wiki_42',\n",
       " '../extracted/AA/wiki_11',\n",
       " '../extracted/AA/wiki_76',\n",
       " '../extracted/AA/wiki_57',\n",
       " '../extracted/AA/wiki_51',\n",
       " '../extracted/AA/wiki_02',\n",
       " '../extracted/AA/wiki_88',\n",
       " '../extracted/AA/wiki_53',\n",
       " '../extracted/AA/wiki_46',\n",
       " '../extracted/AA/wiki_39',\n",
       " '../extracted/AA/wiki_49',\n",
       " '../extracted/AA/wiki_12',\n",
       " '../extracted/AA/wiki_61',\n",
       " '../extracted/AA/wiki_59',\n",
       " '../extracted/AA/wiki_29',\n",
       " '../extracted/AA/wiki_16',\n",
       " '../extracted/AA/wiki_14',\n",
       " '../extracted/AA/wiki_54',\n",
       " '../extracted/AA/wiki_67',\n",
       " '../extracted/AA/wiki_50',\n",
       " '../extracted/AA/wiki_66',\n",
       " '../extracted/AA/wiki_69',\n",
       " '../extracted/AA/wiki_97',\n",
       " '../extracted/AA/wiki_77',\n",
       " '../extracted/AA/wiki_62',\n",
       " '../extracted/AA/wiki_64',\n",
       " '../extracted/AA/wiki_79',\n",
       " '../extracted/AA/wiki_32',\n",
       " '../extracted/AA/wiki_18',\n",
       " '../extracted/AA/wiki_65',\n",
       " '../extracted/AA/wiki_87',\n",
       " '../extracted/AA/wiki_07',\n",
       " '../extracted/AA/wiki_91',\n",
       " '../extracted/AA/wiki_60',\n",
       " '../extracted/AA/wiki_85',\n",
       " '../extracted/AA/wiki_84',\n",
       " '../extracted/AA/wiki_23',\n",
       " '../extracted/AA/wiki_58',\n",
       " '../extracted/AA/wiki_35',\n",
       " '../extracted/AA/wiki_40',\n",
       " '../extracted/AA/wiki_22',\n",
       " '../extracted/AA/wiki_45',\n",
       " '../extracted/AA/wiki_52',\n",
       " '../extracted/AA/wiki_98',\n",
       " '../extracted/AA/wiki_78',\n",
       " '../extracted/AA/wiki_41',\n",
       " '../extracted/AA/wiki_26',\n",
       " '../extracted/AA/wiki_80',\n",
       " '../extracted/AA/wiki_15',\n",
       " '../extracted/AA/wiki_56',\n",
       " '../extracted/AA/wiki_48',\n",
       " '../extracted/AA/wiki_27',\n",
       " '../extracted/AA/wiki_75',\n",
       " '../extracted/AA/wiki_89',\n",
       " '../extracted/AA/wiki_71',\n",
       " '../extracted/AA/wiki_73',\n",
       " '../extracted/AA/wiki_36',\n",
       " '../extracted/AA/wiki_37',\n",
       " '../extracted/AA/wiki_63',\n",
       " '../extracted/AA/wiki_74',\n",
       " '../extracted/AA/wiki_04',\n",
       " '../extracted/AA/wiki_68',\n",
       " '../extracted/AA/wiki_92',\n",
       " '../extracted/AA/wiki_96',\n",
       " '../extracted/AA/wiki_83',\n",
       " '../extracted/AA/wiki_31',\n",
       " '../extracted/AA/wiki_94',\n",
       " '../extracted/AA/wiki_19',\n",
       " '../extracted/AA/wiki_24',\n",
       " '../extracted/AA/wiki_06',\n",
       " '../extracted/AA/wiki_34',\n",
       " '../extracted/AA/wiki_25',\n",
       " '../extracted/AA/wiki_43',\n",
       " '../extracted/AA/wiki_55',\n",
       " '../extracted/AA/wiki_08',\n",
       " '../extracted/AA/wiki_30',\n",
       " '../extracted/AA/wiki_82',\n",
       " '../extracted/AA/wiki_01',\n",
       " '../extracted/AA/wiki_90',\n",
       " '../extracted/AA/wiki_38',\n",
       " '../extracted/AA/wiki_44',\n",
       " '../extracted/AA/wiki_10',\n",
       " '../extracted/AA/wiki_33',\n",
       " '../extracted/AA/wiki_13',\n",
       " '../extracted/AA/wiki_93',\n",
       " '../extracted/AA/wiki_86',\n",
       " '../extracted/AB/wiki_28',\n",
       " '../extracted/AB/wiki_70',\n",
       " '../extracted/AB/wiki_72',\n",
       " '../extracted/AB/wiki_95',\n",
       " '../extracted/AB/wiki_09',\n",
       " '../extracted/AB/wiki_81',\n",
       " '../extracted/AB/wiki_99',\n",
       " '../extracted/AB/wiki_17',\n",
       " '../extracted/AB/wiki_03',\n",
       " '../extracted/AB/wiki_20',\n",
       " '../extracted/AB/wiki_00',\n",
       " '../extracted/AB/wiki_05',\n",
       " '../extracted/AB/wiki_47',\n",
       " '../extracted/AB/wiki_21',\n",
       " '../extracted/AB/wiki_42',\n",
       " '../extracted/AB/wiki_11',\n",
       " '../extracted/AB/wiki_76',\n",
       " '../extracted/AB/wiki_57',\n",
       " '../extracted/AB/wiki_51',\n",
       " '../extracted/AB/wiki_02',\n",
       " '../extracted/AB/wiki_88',\n",
       " '../extracted/AB/wiki_53',\n",
       " '../extracted/AB/wiki_46',\n",
       " '../extracted/AB/wiki_39',\n",
       " '../extracted/AB/wiki_49',\n",
       " '../extracted/AB/wiki_12',\n",
       " '../extracted/AB/wiki_61',\n",
       " '../extracted/AB/wiki_59',\n",
       " '../extracted/AB/wiki_29',\n",
       " '../extracted/AB/wiki_16',\n",
       " '../extracted/AB/wiki_14',\n",
       " '../extracted/AB/wiki_54',\n",
       " '../extracted/AB/wiki_67',\n",
       " '../extracted/AB/wiki_50',\n",
       " '../extracted/AB/wiki_66',\n",
       " '../extracted/AB/wiki_69',\n",
       " '../extracted/AB/wiki_97',\n",
       " '../extracted/AB/wiki_77',\n",
       " '../extracted/AB/wiki_62',\n",
       " '../extracted/AB/wiki_64',\n",
       " '../extracted/AB/wiki_79',\n",
       " '../extracted/AB/wiki_32',\n",
       " '../extracted/AB/wiki_18',\n",
       " '../extracted/AB/wiki_65',\n",
       " '../extracted/AB/wiki_87',\n",
       " '../extracted/AB/wiki_07',\n",
       " '../extracted/AB/wiki_91',\n",
       " '../extracted/AB/wiki_60',\n",
       " '../extracted/AB/wiki_85',\n",
       " '../extracted/AB/wiki_84',\n",
       " '../extracted/AB/wiki_23',\n",
       " '../extracted/AB/wiki_58',\n",
       " '../extracted/AB/wiki_35',\n",
       " '../extracted/AB/wiki_40',\n",
       " '../extracted/AB/wiki_22',\n",
       " '../extracted/AB/wiki_45',\n",
       " '../extracted/AB/wiki_52',\n",
       " '../extracted/AB/wiki_98',\n",
       " '../extracted/AB/wiki_78',\n",
       " '../extracted/AB/wiki_41',\n",
       " '../extracted/AB/wiki_26',\n",
       " '../extracted/AB/wiki_80',\n",
       " '../extracted/AB/wiki_15',\n",
       " '../extracted/AB/wiki_56',\n",
       " '../extracted/AB/wiki_48',\n",
       " '../extracted/AB/wiki_27',\n",
       " '../extracted/AB/wiki_75',\n",
       " '../extracted/AB/wiki_89',\n",
       " '../extracted/AB/wiki_71',\n",
       " '../extracted/AB/wiki_73',\n",
       " '../extracted/AB/wiki_36',\n",
       " '../extracted/AB/wiki_37',\n",
       " '../extracted/AB/wiki_63',\n",
       " '../extracted/AB/wiki_74',\n",
       " '../extracted/AB/wiki_04',\n",
       " '../extracted/AB/wiki_68',\n",
       " '../extracted/AB/wiki_92',\n",
       " '../extracted/AB/wiki_96',\n",
       " '../extracted/AB/wiki_83',\n",
       " '../extracted/AB/wiki_31',\n",
       " '../extracted/AB/wiki_94',\n",
       " '../extracted/AB/wiki_19',\n",
       " '../extracted/AB/wiki_24',\n",
       " '../extracted/AB/wiki_06',\n",
       " '../extracted/AB/wiki_34',\n",
       " '../extracted/AB/wiki_25',\n",
       " '../extracted/AB/wiki_43',\n",
       " '../extracted/AB/wiki_55',\n",
       " '../extracted/AB/wiki_08',\n",
       " '../extracted/AB/wiki_30',\n",
       " '../extracted/AB/wiki_82',\n",
       " '../extracted/AB/wiki_01',\n",
       " '../extracted/AB/wiki_90',\n",
       " '../extracted/AB/wiki_38',\n",
       " '../extracted/AB/wiki_44',\n",
       " '../extracted/AB/wiki_10',\n",
       " '../extracted/AB/wiki_33',\n",
       " '../extracted/AB/wiki_13',\n",
       " '../extracted/AB/wiki_93',\n",
       " '../extracted/AB/wiki_86',\n",
       " '../extracted/AC/wiki_28',\n",
       " '../extracted/AC/wiki_09',\n",
       " '../extracted/AC/wiki_17',\n",
       " '../extracted/AC/wiki_03',\n",
       " '../extracted/AC/wiki_20',\n",
       " '../extracted/AC/wiki_00',\n",
       " '../extracted/AC/wiki_05',\n",
       " '../extracted/AC/wiki_21',\n",
       " '../extracted/AC/wiki_11',\n",
       " '../extracted/AC/wiki_02',\n",
       " '../extracted/AC/wiki_12',\n",
       " '../extracted/AC/wiki_29',\n",
       " '../extracted/AC/wiki_16',\n",
       " '../extracted/AC/wiki_14',\n",
       " '../extracted/AC/wiki_32',\n",
       " '../extracted/AC/wiki_18',\n",
       " '../extracted/AC/wiki_07',\n",
       " '../extracted/AC/wiki_23',\n",
       " '../extracted/AC/wiki_22',\n",
       " '../extracted/AC/wiki_26',\n",
       " '../extracted/AC/wiki_15',\n",
       " '../extracted/AC/wiki_27',\n",
       " '../extracted/AC/wiki_04',\n",
       " '../extracted/AC/wiki_31',\n",
       " '../extracted/AC/wiki_19',\n",
       " '../extracted/AC/wiki_24',\n",
       " '../extracted/AC/wiki_06',\n",
       " '../extracted/AC/wiki_25',\n",
       " '../extracted/AC/wiki_08',\n",
       " '../extracted/AC/wiki_30',\n",
       " '../extracted/AC/wiki_01',\n",
       " '../extracted/AC/wiki_10',\n",
       " '../extracted/AC/wiki_13']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('../extracted/*/*')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39b0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "x= pl.scan_parquet('./000_00000.parquet')\n",
    "#x.head().collect()\n",
    "x = x.filter((pl.col('token_count')>100)&(pl.col('score')>2.5)&(pl.col('language')=='en'))\n",
    "x = x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c837dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the translated text above, these terms all get the articles, however, in the test, they all don't.\n",
      "i.e.:la astronomía, y en las matemáticas. También, trabajaron en la agricultura y el comercio\"\n",
      "in the test, it corrected me, so I'm a bit confused:\n",
      "Could you please let me know when you can/cannot use the article \"le,la,las\" etc in front of these:\n",
      "The test examples:\n",
      "Kwizbot Los aztecas destacaron en astronomía,\n",
      "You Los aztecas destacan en la astronomía,\n",
      "You could also say: Los aztecas destacaron en la astronomía,\n",
      "Kwizbot También, trabajaron en la agricultura\n",
      "You También, trabajaron en agricultura\n",
      "[note: here I dropped the article and it didn't correct me]\n",
      "Well done! and in mathematics.\n",
      "Your answer matched mine: y en matemáticas.\n",
      "Thank you for your help and have a great day!\n",
      "Freeform Writing Exercise A2\n",
      "Four years later, and still no answer to this question. I had the same questions. It appears to me using various online resources, grammar correcting sites, and ChatGPT that the use of articles is optional in some of these situations. It is not clear at all when they HAVE to be used. But if in doubt, it will be the opposite of whatever I write or say!\n",
      "Sign in to submit your answer\n",
      "Don't have an account yet? Join today\n",
      "Test your Spanish to the CEFR standard\n"
     ]
    }
   ],
   "source": [
    "print(x.filter(x['language_score']<0.7)['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c4a658-9579-4077-9502-228bf119dad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our comprehensive guide on Argon2 vs PBKDF2, two of the most popular password hashing algorithms in the world of cybersecurity. In this article, we’ll explore the key differences between Argon2 and PBKDF2, examining their security features, performance, and best use cases. Whether you are a developer looking to enhance your application’s security or simply curious about the best practices for protecting your sensitive data, you’ll find valuable insights here. Join us as we break down the strengths and weaknesses of each hashing method, helping you make an informed decision for your password management needs.\n",
      "What is Argon2\n",
      "Argon2 is a modern cryptographic hash function specifically designed for password hashing and key derivation. Developed in 2015 and the winner of the Password Hashing Competition (PHC), Argon2 focuses on security against both brute-force attacks and specialized hardware attacks. It comes in two variants: Argon2d, which is resistant to GPU cracking attacks, and Argon2i, which is optimized for side-channel attack resistance. The flexibility in configuring memory usage, time cost, and parallelism makes Argon2 a powerful choice for developers looking to enhance password security.\n",
      "Popular Use Cases of Argon2\n",
      "Argon2 is increasingly being adopted across various applications due to its robust security features. Some of the most popular use cases include:\n",
      "- Web Applications: Many web applications utilize Argon2 for securely storing user passwords. It ensures that even if the database is compromised, the passwords remain protected.\n",
      "- Mobile Apps: Developers of mobile applications use Argon2 to enhance the security of sensitive user data, such as authentication tokens and personal information.\n",
      "- Cryptocurrency Wallets: With the rise of cryptocurrencies, Argon2 is employed in wallets to secure private keys, adding an extra layer of protection against unauthorized access.\n",
      "What is PBKDF2\n",
      "PBKDF2, or Password-Based Key Derivation Function 2, is a widely used key derivation function that applies a pseudorandom function, such as HMAC, to a password along with a salt and an iteration count. Introduced in 2000 and specified in RFC 2898, PBKDF2 is designed to make password cracking more difficult by increasing the time it takes to compute the hash. While it is an established and trusted method, its reliance on parameters like iteration count makes it less flexible compared to newer alternatives like Argon2.\n",
      "Popular Use Cases of PBKDF2\n",
      "PBKDF2 remains a popular choice in various applications, including:\n",
      "- Legacy Systems: Many older systems still utilize PBKDF2 for password hashing due to its long-standing reputation and established security practices.\n",
      "- Secure File Storage: Applications that require secure file storage use PBKDF2 for encrypting file access keys, ensuring that unauthorized users cannot easily gain access.\n",
      "- Web Services: Various web services implement PBKDF2 for user authentication, providing a reliable means of protecting user credentials.\n",
      "Argon2 vs PBKDF2: Understanding the Differences\n",
      "While both Argon2 and PBKDF2 serve the same ultimate purpose of securely hashing passwords, they differ significantly in design and performance.\n",
      "Argon2 offers more advanced security features, including memory-hardness properties, which make it resistant to GPU and ASIC attacks. PBKDF2, while secure, relies primarily on increasing iteration counts, which can be less effective against modern hardware.\n",
      "Argon2 is designed to be highly configurable, allowing developers to adjust memory and processing time according to their needs. This adaptability makes it well-suited for various environments. On the other hand, PBKDF2 can be less flexible, as it primarily focuses on the number of iterations, which might not be sufficient against powerful hardware.\n",
      "Adoption and Support\n",
      "Argon2 is gaining traction in modern applications, supported by many programming languages and frameworks. PBKDF2 has been widely adopted for years, giving it a solid base in established systems. However, as security standards evolve, Argon2's advantages may lead to a decline in PBKDF2's popularity.\n",
      "In the debate of Argon2 vs PBKDF2, Argon2 emerges as the more advanced and flexible option for password hashing and key derivation. Its innovative design prioritizes security against contemporary threats, making it an ideal choice for new applications. However, PBKDF2's long-standing reliability means it will continue to be utilized in legacy systems. Ultimately, the choice between Argon2 and PBKDF2 should be guided by specific project requirements, security needs, and future scalability considerations. As cybersecurity threats continue to evolve, opting for the most robust hashing algorithm is crucial in safeguarding sensitive user data.\n"
     ]
    }
   ],
   "source": [
    "print(x.filter(x['score']<2.7)['text'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7327755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Google API Key not found. Please set the GOOGLE_API_KEY environment variable or define it in the script.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash-8b\"\n",
    "\n",
    "ALLOWED_TOPICS = [\"Art\", \"History\", \"Science\", \"General knowledge\"]\n",
    "ALLOWED_COMPLEXITY = [\"elementary\", \"middle school\", \"high school\", \"university degree\", \"doctorate\"]\n",
    "\n",
    "def classify_text_with_gemini(text_to_classify):\n",
    "    \"\"\"\n",
    "    Uses Gemini to classify text into topic and complexity.\n",
    "\n",
    "    Args:\n",
    "        text_to_classify (str): The text content to classify.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (topic, complexity) or (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    # Ensure text is not empty or just whitespace\n",
    "    if not text_to_classify or text_to_classify.isspace():\n",
    "        print(\"Warning: Skipping empty or whitespace-only text.\")\n",
    "        return None, None\n",
    "\n",
    "    # Limit text length to avoid exceeding model limits (adjust as needed)\n",
    "    max_length = 15000 # Example limit, check model's token limits if necessary\n",
    "    if len(text_to_classify) > max_length:\n",
    "        text_to_classify = text_to_classify[:max_length] + \"...\" # Truncate\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Analyze the following text and determine its primary topic and the educational level typically required to understand it.\n",
    "\n",
    "Text:\n",
    "\"{text_to_classify}\"\n",
    "\n",
    "Instructions:\n",
    "1. Choose the *single best* topic from this list: {ALLOWED_TOPICS}\n",
    "2. Choose the *single most appropriate* complexity level from this list: {ALLOWED_COMPLEXITY}\n",
    "3. Provide your answer ONLY in the following JSON format:\n",
    "   {{\"topic\": \"SELECTED_TOPIC\", \"complexity\": \"SELECTED_COMPLEXITY\"}}\n",
    "\n",
    "Example Response:\n",
    "{{\"topic\": \"Science\", \"complexity\": \"high school\"}}\n",
    "\n",
    "Output ONLY the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL_NAME)\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                # candidate_count=1, # Ensure only one response candidate\n",
    "                # max_output_tokens=50, # Limit output size\n",
    "                temperature=0.1 # Lower temperature for more deterministic classification\n",
    "            )\n",
    "            )\n",
    "\n",
    "        raw_response_text = response.text.strip()\n",
    "        if raw_response_text.startswith(\"```json\"):\n",
    "            raw_response_text = raw_response_text[7:] # Remove ```json\n",
    "        if raw_response_text.endswith(\"```\"):\n",
    "            raw_response_text = raw_response_text[:-3] # Remove ```\n",
    "        raw_response_text = raw_response_text.strip() # Clean whitespace again\n",
    "\n",
    "        # Parse the JSON response\n",
    "        result = json.loads(raw_response_text)\n",
    "        topic = result.get(\"topic\")\n",
    "        complexity = result.get(\"complexity\")\n",
    "\n",
    "        # Validate response against allowed categories\n",
    "        if topic not in ALLOWED_TOPICS:\n",
    "            print(f\"Warning: Received invalid topic '{topic}'. Setting to None.\")\n",
    "            topic = None\n",
    "        if complexity not in ALLOWED_COMPLEXITY:\n",
    "            print(f\"Warning: Received invalid complexity '{complexity}'. Setting to None.\")\n",
    "            complexity = None\n",
    "\n",
    "        return topic, complexity\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Failed to decode JSON response: {raw_response_text}\")\n",
    "        return \"Error: JSON Decode\", \"Error: JSON Decode\"\n",
    "    except ValueError as ve: # Handle potential errors from model generation (e.g., blocked content)\n",
    "         print(f\"Error: Gemini API returned ValueError (potentially blocked content or invalid response structure). Raw response: {response.text if 'response' in locals() else 'N/A'}. Details: {ve}\")\n",
    "         return \"Error: API Value\", \"Error: API Value\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return \"Error: API Call\", \"Error: API Call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41def585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This project is solving the Asteroid Watchers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Life of a Sand Grain\\nTHE LIFE OF A SAND GRAIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The internet of things, a system of interrelat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An archive photo of an Egyptian mummy - Reuter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The faith of the Christ-God is a living parado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Extreme weather events are generally associate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>In diary studies, people provide frequent repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A 10 minutes read to dive into the REFLOW proj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>More than lullabies and swaying. Kindermusik i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>In a lot of construction sites this time of ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   This project is solving the Asteroid Watchers ...\n",
       "1   Life of a Sand Grain\\nTHE LIFE OF A SAND GRAIN...\n",
       "2   The internet of things, a system of interrelat...\n",
       "3   An archive photo of an Egyptian mummy - Reuter...\n",
       "4   The faith of the Christ-God is a living parado...\n",
       "..                                                ...\n",
       "95  Extreme weather events are generally associate...\n",
       "96  In diary studies, people provide frequent repo...\n",
       "97  A 10 minutes read to dive into the REFLOW proj...\n",
       "98  More than lullabies and swaying. Kindermusik i...\n",
       "99  In a lot of construction sites this time of ye...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['text'])\n",
    "\n",
    "df['text'] = x['text'][0:100].to_list()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "817c964f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('General knowledge', 'elementary')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text_with_gemini('Questo è un racconto per bambini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e93e5ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Text: 100%|████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "MAX_WORKERS = 8\n",
    "results = [None] * len(df) # Pre-allocate list to store results in order\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "\n",
    "    future_to_index = {\n",
    "        executor.submit(classify_text_with_gemini, text): i\n",
    "        for i, text in enumerate(df['text'])\n",
    "    }\n",
    "\n",
    "    # Process futures as they complete, using tqdm for progress\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_index), total=len(df), desc=\"Classifying Text\"):\n",
    "        index = future_to_index[future]\n",
    "        try:\n",
    "            # Get the result tuple (topic, complexity)\n",
    "            result_tuple = future.result()\n",
    "            results[index] = result_tuple\n",
    "        except Exception as exc:\n",
    "            print(f'Row {index} generated an exception: {exc}')\n",
    "            # Store error indication in results for the specific row\n",
    "            results[index] = (\"Error: Future Exception\", \"Error: Future Exception\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06282ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412145408"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['token_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce59cbad-3c77-40f0-a165-c8b6d11c3226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 233/233 [00:04<00:00, 54.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      filename                             title  \\\n",
      "0       ./extracted/AB/wiki_63                      Michel Debré   \n",
      "1       ./extracted/AB/wiki_63                    Michel Barnier   \n",
      "2       ./extracted/AB/wiki_63                 Hervé de Charette   \n",
      "3       ./extracted/AB/wiki_63            Sweet and Sour (movie)   \n",
      "4       ./extracted/AB/wiki_63                       Dafydd Iwan   \n",
      "...                        ...                               ...   \n",
      "367504  ./extracted/AC/wiki_29                   Jonathan Majors   \n",
      "367505  ./extracted/AC/wiki_29                     Bajwa (caste)   \n",
      "367506  ./extracted/AC/wiki_29             2025 Tibet earthquake   \n",
      "367507  ./extracted/AC/wiki_29  Internment of Japanese Canadians   \n",
      "367508  ./extracted/AC/wiki_29                       Leap second   \n",
      "\n",
      "                                                     text  \\\n",
      "0       Michel Jean-Pierre Debré (; 15 January 1912 – ...   \n",
      "1       Michel Bernard Barnier (born 9 January 1951) i...   \n",
      "2       Hervé de Charette (born 30 July 1938) is a Fre...   \n",
      "3       Sweet and Sour () is a 1963 French Italian com...   \n",
      "4       Dafydd Iwan Jones (born 24 August 1943) is a W...   \n",
      "...                                                   ...   \n",
      "367504  Jonathan Michael Majors (born September 7, 198...   \n",
      "367505  ‌‌Bajwa is an Principle‌‌‌ caste or Patti‌ () ...   \n",
      "367506  On 7 January 2025, at 09:05 CST, an earthquake...   \n",
      "367507  From 1942 to 1949, Canada forced over 22,000 J...   \n",
      "367508  A leap second is adding one extra second to ti...   \n",
      "\n",
      "        number_of_characters  number_of_words topic  text_quality  \n",
      "0                        281               49   N/A             0  \n",
      "1                       1843              309   N/A             0  \n",
      "2                        844              147   N/A             0  \n",
      "3                        269               39   N/A             0  \n",
      "4                        347               59   N/A             0  \n",
      "...                      ...              ...   ...           ...  \n",
      "367504                  1108              176   N/A             0  \n",
      "367505                   862              131   N/A             0  \n",
      "367506                  2954              488   N/A             0  \n",
      "367507                  1766              294   N/A             0  \n",
      "367508                  1109              193   N/A             0  \n",
      "\n",
      "[367509 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "data_rows = []\n",
    "\n",
    "for file_path in tqdm(files):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                title = record.get(\"title\", \"\")\n",
    "                text = record.get(\"text\", \"\")\n",
    "                #tfidf_topics = detect_topics_by_frequency(text)\n",
    "                \n",
    "                #topic_label = \", \".join(tfidf_topics)\n",
    "                \n",
    "                data_rows.append({\n",
    "                    \"filename\": file_path,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"number_of_characters\": len(text),\n",
    "                    \"number_of_words\": len(text.split()),\n",
    "                    \"topic\": 'N/A',\n",
    "                    \"text_quality\": 0,\n",
    "                    \n",
    "                })\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data_rows)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15a7305e-aaf2-4adf-ba44-13e7eef38bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45825104.28571429"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['number_of_words'].sum()/0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1679cf-94e7-41d7-9d48-2873ef3f20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('simple_english.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ab8c801-cb19-4586-b0ca-139287341841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michel Jean-Pierre Debré (; 15 January 1912 – 2 August 1996) was a French politician. He was the first Prime Minister of the French Fifth Republic. He has been called the \"father\" of the current Constitution of France. He worked with President Charles de Gaulle from 1959 to 1962. '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ac85cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824213e-104c-478e-9346-8e1c51428b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "336edc7d-8e92-4ba1-b2b4-965a1a276027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "enc = PreTrainedTokenizerFast.from_pretrained('./dev/data/simple_english.parquet/custom_tokenizer')\n",
    "\n",
    "with open('./dev/data/simple_english.parquet/simple_english.parquet_custom_train_000041.bin', \"rb\") as f:\n",
    "    # first read the header, which is 256 int32 integers (4 bytes each)\n",
    "    header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
    "    ntok = header[2] # number of tokens (claimed)\n",
    "    # the rest of it are tokens, stored as uint16\n",
    "    tokens = np.frombuffer(f.read(), dtype=np.uint32)\n",
    "    \n",
    "    \n",
    "decoded_text = enc.decode(tokens[100:200]).replace(\" \", \"\").replace(\"Ġ\", \" \").replace(\" ##\", \"\").replace('Ċ','\\n')\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c070555-71e9-47c9-bea5-27001a0efd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adbfec-eceb-430e-8175-06a4e46ac158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e04b7-845f-47a1-b56d-b3b54507d758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9393505-0cbf-4f4b-b893-b638982bd25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0fa8b1ae-11fb-4f93-9896-edbc6f9c9c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "afc136aa-6cbf-422c-a2e7-87ac30aefa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " originated from English speaking countries. It finds seldom use in other metricated Commonwealth Nations.</s></s></s> Shohreh Solati (, born Fatemeh Solati on January 4, 1959, in Tehran) is an Iranian singer. She is among the most active and prolific Iranian female singers. Since the Iranian Revolution, she has continued her music career outside the country.\n",
      "She is divorced with a daughter and lives in Los Angeles, California.</s></s>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2ed3c-7156-4db7-929e-e99adafdc346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
